# The-next-tech-capitol
It was a group project and we wanted to see how has Amazon’s HQ2 selection process impacted the housing market? In other words, has consumer speculation about HQ2 locations driven up (or down) market prices in the finalist cities? And if so, what was the relationship between speculation and price change?
Initial Concept: At project start, we wanted to ask the question – how has Amazon’s HQ2 selection process impacted the housing market? In other words, has consumer speculation about HQ2 locations driven up (or down) market prices in the finalist cities? And if so, what was the relationship between speculation and price change?

Concept Revision: Near the end of the project, new guidance modified the assignment such that the project was not sufficient if the project did answer a specific business question. In response, we retooled our data to answer a new question: In what city should a client launch a business, who desires access to a technology dense market (like Silicon Valley) without cost-prohibitive pricing? In other words, our goal became: Identify which of the twenty finalist cities is most likely to become the next US tech hub?

Data Sources: Quantifying public speculation became the first major hurdle. An extensive amount of beliefs about finalist cities’ eligibility for selection as Amazon HQ2 has flooded social media. We chose twitter as our public speculation data source. Quantifying changes in the housing market was a simpler selection decision. Zillow’s research center maintains extensive records of median dollar per square-foot (MDSF) per city, dating back to 1997.

Storing Data: We created a table in a MySQL database via cloud computing platform, PythonAnywhere. In addition, we used tweepy, a Twitter API wrapper, to query tweets relevant to HQ2 and each of the finalist cities; we stored these results in the MySQL table for later analysis. This data collection was executed once per day via PythonAnywhere, only storing tweets with a new tweet_id, as to prevent duplicates. Zillow’s MDSF data was stored in a csv file. Unlike tweets, the MDSF data was not subject to daily change.    

Data Limitations: Twitter’s API allows for easy collection of tweets in the proceeding 5-10 days, on average. However, accessing tweets as far back as January 2018 (when the 20 finalists cities were announced by Amazon) was not possible via the API. In a future project, we might use web-scraping to collect older tweets (not available to the API.) However, Twitter uses Ajax, meaning that page contents load asynchronously, a browser emulator (such as Selenium) would likely be required – and would be computationally intensive. Ergo, we limited our analysis of consumer sentiment to September through November, ending on the 13th when dual HQ2 locations were announced; subsequent tweets are biased and could not be used.

Sentiment Analysis: Volume of tweets, alone, is not sufficient to determine if a city was or was not a good candidate for HQ2. We needed to categorize the tweets as positive or negative. However, this specific task had not been done before. Typically, sentiment analysis is sensitive to words like “terrific” and “horrible” – which reflect the emotion behind the text. However, we needed an algorithm capable of detecting different features, such as “terrific public transportation” and “horrible traffic.” 
Corpus Creation: We selected 120 tweets, regarding finalist cities and HQ2, between January and August 2018. This data would not be collected via the twitter API so there was no risk of training the algorithm on the same data that it would later classify (heavily biased.) We took extreme care to collect three positive and three negative tweets per city. If there was any imbalance, the algorithm might determine that a city’s a name is weighted positively or negatively. Additionally, we searched extensively for ideal examples of positive or negative sentiment in our task-specific-context. 

Corpus Limitations: Many tweets (before and after September 2018) contain little or no justifications defending their position; i.e. “Amazon, please come to Denver!” This fictitious tweet doesn’t provide any reasons why Amazon should locate HQ2 in Denver. We omitted tweets like this from our training data. In retrospect, we could have added three neutral tweets per city to the training data. This would have made it easier for our algorithm to handle tweets that offer no justifications supporting their position. Additionally, creating a corpus was extremely labor intensive. We had to ensure that no city was over or underrepresented in training data, and that the ratio of positive to negative tweets was balanced. One product of this high labor cost, was that our training data only included 120 samples. A higher number likely would have yielded better results.

Tweet Pre-classification processing: First, tweets were divided into training and testing sets via sklearn library’s train-test-split function. Then tweets were stripped of stop-words (‘the’, ‘and’) and punctuation, returning a list of words; this process is known as “tokenization.” Then a bag of words (BOW) was created from the training data - essentially a python dictionary, where the keys are a python set of the total vocabulary and the values set to zero by default. Next Term Frequency-Inverse Document Frequency (TF-IDF) was applied to the BOW. In essence, each tweet in the training data was vectorized (same length as the BOW.) Most elements were set to 0 (meaning a given word did not occur in the tweet), and if a given word occurred one or more times, the element was set to 1. Then these vectors were summed, each element divided by the total number of training tweets, and inversed (1 divided by the previous value.) Lastly the term’s BOW values were set to the Term Frequency multiplied by the natural log of the inverse document frequency. Simply put, each word in the BOW was weighted and smoothed such that the more frequently a given word occurred amongst all the tweets, the lower the weight. i.e. “horrible” has more weight than “bad.”  

Logistic Regression: After preprocessing, a logistic regression model was trained on training data, given labels as 0 or 1 as dependent variable and vectorized tweets with TF-IDF weights as the independent variable. When predicting classification on testing data tweets, the precision was found to be 1.0 and recall 0.48. We compared logistic regression to other models, such as Random Forest and Naïve Bayes but found that logistic regression had the highest precision value. Our subsequent analysis relied heavily upon precision of positive classified tweets but was less interested in negative classified tweets. 

Sentiment score: Our first concern was controlling for city size relationship to volume of tweets. Naturally, bigger cities will produce more tweets than smaller cities as a function of their population size. Second, it’s assumed in this project that tweets can, in some way, affect housing prices. The exact relationship will be discussed later; however, it’s implicit that the more often a tweet is read, the greater an impact it will make on the market, whatever that be. Ergo, our score sums the number of followers of all tweets regarding a finalist city and divides that figure by the population. This number, the circulation coefficient, represents the relative “buzz” a given city receives, scaled for population size. Next, the circulation coefficient is multiplied by the ratio of positive tweets to total tweets of that city. Because the precision value for positive tweets was extremely high, but the recall value was moderate to low, we had to restrict our calculations to use of positive tweets and total tweets. In other words, there were few false positives, but several false negatives, and in consequence, tweets classified as negative have much lower fidelity than those classified as positive. This assumes that there is no pattern among False negatives across the 20 finalist cities. Atlanta was found to have the highest sentiment score.

Housing Prices as a Proxy for Technology Jobs: To adequately answer our client’s question, we are not only interested in the city with the highest sentiment score; we are also concerned with which cities have the greatest potential to become a future Silicon Valley. If a city rapidly attracts migrants beyond what available housing can support, then the demand and price for its housing will rise. And this increased demand must be, at least in part, due to growth of its job opportunities and its industries. One assumption that this analysis makes is that the technology sector of a given city is not exempt from the overall growth of its industries. In summary, we conclude that a rapid increase in technology jobs is an excellent predictor of a city’s potential to become a future tech hub and that a rapid increase in housing prices is a suitable proxy for an increase in technology jobs.

Unexpected Growth: Returning to our initial concept, we wanted to quantify what amount of growth was due to speculation regarding HQ2. If we simply computed the difference between January and November 2018, how could we say that it definitively was all due to Amazon? What fraction of the growth was destined to occur, regardless of HQ2? To answer this question, we trained a linear regression model on our MDSF data from dates January 2010 to December 2017. This selection omitted outlier data, subject to the 2008 financial crisis. In the case of Boston, the r-squared was found to be 0.97, showing that a linear model is quite suitable for some cities. Next, we forecasted the expected growth over the course of 2018 via our linear regression model. And lastly, we compared the percent changes of both the expected and observed growth (from January to November 2018,) subtracting the expected from the observed. If the resulting value was near zero, then there was no growth (or loss) to attribute to Amazon. In the case of Atlanta, the expected growth was app. 3% and the observed growth was app. 8.6%. The unexpected growth (difference) was 5.6% for Atlanta. In the context of our revised concept, unexpected growth is extremely useful as it reveals rapid departures from longstanding trends. The more extreme (and positive) the departure from the trend, the higher the likelihood that a city will become a future technology hub. 

Ranking and Conclusion: To balance the importance of sentiment score and unexpected growth, we multiplied these coefficients together and rank ordered the results. Atlanta was found to have score of 1042, whereas the next closest ranked city, Miami, fell short of 100. The extreme value that Atlanta received, shows that it is growing quite rapidly, indicating that technology jobs are sharply on the rise, and that it produced an extensive number of positive tweets (relative to its negative tweets and its population size.) These positive tweets reflect numerous justifications why Atlanta would have been an ideal city for Amazon’s HQ2. Ultimately, Amazon did not select Atlanta, however, the metadata produced by Amazon’s “HQ2 tournament” can be retooled in a variety of ways to optimize various business decisions that otherwise, could only be solved through more exhaustive research (such as where to launch a startup.) In conclusion, we recommend that our client launch their new business in Atlanta, GA, as it perfectly solves their business problem. 
